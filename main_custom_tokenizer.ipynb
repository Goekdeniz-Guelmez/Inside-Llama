{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Introduction\n",
    "\n",
    "Welcome to the labyrinth of \"Inside Llama,\" where we unravel the complexities of Meta's Llama 3 model. This notebook is a testament to the power of precision, knowledge, and the relentless pursuit of perfection. It is designed for those who aspire not just to understand, but to master the inner workings of one of the most sophisticated language models in existence.\n",
    "\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook is divided into several critical sections. Each one is a rung on the ladder to dominance over the machine learning landscape.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "Our mission is clear: to construct and train the Llama 3 model from scratch, employing a character-based tokenizer as our tool of choice. Inspired by the teachings of Andrew Karpathy, this tokenizer is the key to unlocking the full potential of our model. Should your preferences lean towards tradition, the original tokenizer from the Huggingface Hub is but a switch away, courtesy of the `transformers` library.\n",
    "\n",
    "\n",
    "### Architectural Blueprint\n",
    "\n",
    "Before we embark on our journey, let us pause to appreciate the architectural elegance of Llama 3, as depicted in the following diagram:\n",
    "\n",
    "<img src=\"./Llama-Architecture.png\" alt=\"Llama Architecture\" width=\"500\">\n",
    "\n",
    "\n",
    "This diagram is more than a mere image; it is a blueprint of our conquest:\n",
    "\n",
    "1. **Input Tokens**: The journey begins with the input tokens, the raw material fed into the model.\n",
    "\n",
    "2. **Embeddings**: These tokens are transformed into embeddings, the foundation upon which our model is built.\n",
    "\n",
    "3. **Transformer Block**: The core of our architecture, the Transformer Block, where the magic happens:\n",
    "   - **Multi-Head-Self-Attention**: This component employs Grouped-Multi-Query-Attention with KV-Cache, a sophisticated mechanism for attention across multiple heads. We will not be using the KV-Cache in this notebook.\n",
    "   - **RMS Norm**: Normalization is applied to ensure stability and efficiency.\n",
    "   - **SwiGLU Activated MLP Layer**: A crucial layer where activation functions breathe life into our model.\n",
    "   - **RMS Norm**: Another layer of normalization to maintain equilibrium.\n",
    "   - **Residual Connections**: These connections ensure that information flows smoothly through the network without loss.\n",
    "   \n",
    "4. **Output Tokens**: The culmination of our efforts, the output tokens, are derived from the softmax probabilities and argmax operations, translating the model's predictions into tangible results.\n",
    "\n",
    "This is the architecture that will guide us, the blueprint that will lead us to mastery. As we proceed through the notebook, each section will bring us closer to fully understanding and harnessing the power of Llama 3.\n",
    "\n",
    "Prepare yourself for a journey of discovery, precision, and unparalleled mastery. Welcome to \"Inside Llama\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "from utils import createPlot, createLossPlot, plot_lm_head_output, plot_probs_or_logits, plot_mask_tensor, plot_intermediate_attention, parse_parameters_from_file, visualize_parameters, save_model_parameters_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "\n",
    "- **dim: int = 16 # 4096**: The core dimensions of our model. In the realm of possibilities, we set it to 16, though it could scale to 4096 in more ambitious undertakings like the Llama3 8B model.\n",
    "\n",
    "- **n_layers: int = 6 # 32**: The number of Decoder-Transformer Layers. We start with 6, but the ceiling is our device were training it on, Llama3 8B uses 32.\n",
    "\n",
    "- **n_heads: int = 8 # 32**: The count of Single Attention Heads in our Multi-Head Attention mechanism. We begin with 8, yet can extend to Llama3 8B's 32, each head enhancing our model's perceptive power.\n",
    "\n",
    "- **n_kv_heads: Optional[int] = 8 # 8**: The number of key-value heads in the attention mechanism. Set at 8, a balanced number ensuring efficiency and depth.\n",
    "\n",
    "- **vocab_size: int = -1**: The vocabulary size, as yet undefined.\n",
    "\n",
    "- **multiple_of: int = 24**: This ensures the SwiGLU hidden layer size is a multiple of a large power of 2, originally 256. It’s a move of strategic alignment, ensuring optimal performance.\n",
    "\n",
    "- **ffn_dim_multiplier: Optional[float] = None**: The multiplier for the Feed-Forward Network dimension, currently undefined, allowing for dynamic scaling as needed.\n",
    "\n",
    "- **rms_norm_eps: float = 1e-5**: The epsilon value for RMS normalization, a fine-tuned parameter ensuring stability and precision in our model’s calculations.\n",
    "\n",
    "- **max_batch_size: int = 6**: The maximum batch size, set to 6. A modest start, with the potential for scaling as our model’s appetite grows.\n",
    "\n",
    "- **max_seq_len: int = 32**: The maximum sequence length, a defining parameter that caps our input sequences at 32, ensuring manageable complexity.\n",
    "\n",
    "- **plot = False**: The Plot property, set to False for now. When we choose to visualize the values within our model, we’ll switch it on, revealing the intricate workings beneath the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim: int = 24\n",
    "n_layers: int = 8\n",
    "n_heads: int = 12\n",
    "n_kv_heads: Optional[int] = 12\n",
    "vocab_size: int = -1\n",
    "multiple_of: int = 24\n",
    "ffn_dim_multiplier: Optional[float] = None\n",
    "rms_norm_eps: float = 1e-5\n",
    "max_batch_size: int = 6\n",
    "max_seq_len: int = 16\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's shift our attention (pun intended) to our Tokenizer and our Dataset.\n",
    "\n",
    "In this segment, we see the elegance of our strategy unfold:\n",
    "\n",
    "1. **Loading the Dataset**: We begin by drawing in our raw data, a fundamental step that brings us closer to the heart of our endeavor.\n",
    "\n",
    "2. **Creating the Tokenizer**: \n",
    "    - **Character Analysis**: We meticulously analyze the unique characters within our dataset, understanding the building blocks of our linguistic universe.\n",
    "    - **Vocab Insights**: We reveal the vocabulary size, a metric of our model’s breadth and comprehension.\n",
    "    - **Mapping Characters**: Two critical dictionaries are crafted:\n",
    "        - **`stoi`**: Maps characters to their respective indices.\n",
    "        - **`itos`**: Reverses the map, from indices back to characters.\n",
    "    - **Tokenization and Detokenization**: We define our lambda functions, transforming strings to sequences of indices and back, ensuring seamless transitions between raw text and numerical representations.\n",
    "\n",
    "3. **Padding ID**: Finally, we identify our padding ID, a key player in managing sequences of varying lengths, ensuring consistency and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... Loading Dataset\")\n",
    "with open(\"tiny-shakespear.txt\", \"r\") as file:\n",
    "    dataset = file.read()\n",
    "\n",
    "print(\"... Initializing Tokenizer\")\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "print(\"... Pre-tokenizing\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "print(\"... Training custom Tokenizer\")\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=30522,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\"]\n",
    ")\n",
    "tokenizer.train_from_iterator([dataset], trainer=trainer)\n",
    "\n",
    "print(\"... Saving Tokenizer\")\n",
    "tokenizer.save(\"custom_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... Loading Tokenizer\")\n",
    "tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
    "\n",
    "# Fetching and displaying the vocab size from the tokenizer\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Assuming you want to print the ID for padding if you have added one\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\") if \"[PAD]\" in tokenizer.get_vocab() else None\n",
    "print(f\"Padding ID: {pad_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "create_a_instace_of_layernorm = RMSNorm(3)\n",
    "what_is_layer_norm = create_a_instace_of_layernorm(torch.Tensor([1.0, 5.6, -8.3]))\n",
    "print(f\"the input numbers are [1.0, 5.6, -8.3], and after the Layernorm are: {what_is_layer_norm.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now examine the precomputation and application of frequency components in our model.\n",
    "\n",
    "Here, we witness the meticulous orchestration of frequency component precomputation and their application to query and key tensors in the attention mechanism:\n",
    "\n",
    "1. **Precomputing Frequency Components (`precompute_freqs_cis`)**:\n",
    "    - **Parameters**:\n",
    "        - `dim`: The dimensionality of the input.\n",
    "        - `end`: The sequence length.\n",
    "        - `theta`: A scaling factor, defaulting to `10000.0`.\n",
    "    - **Frequency Calculation**:\n",
    "        - Generates frequency values by scaling the inverse powers of `theta` raised to the fraction of dimensionality.\n",
    "        - `freqs`: This array of frequencies is derived from a linear space, normalized by `dim`.\n",
    "    - **Time Steps**:\n",
    "        - `t`: A range tensor from `0` to `end`, representing time steps.\n",
    "        - `freqs` is then expanded across these time steps using an outer product.\n",
    "    - **Complex Frequency Representation**:\n",
    "        - `freqs_cis`: Converts these frequency values into a complex form using polar coordinates, where the magnitude is `1` and the phase angle is given by `freqs`.\n",
    "\n",
    "2. **Reshaping for Broadcast (`reshape_for_broadcast`)**:\n",
    "    - **Parameters**:\n",
    "        - `freqs_cis`: The precomputed complex frequency tensor.\n",
    "        - `x`: The tensor to which the frequencies will be applied.\n",
    "    - **Shape Adjustment**:\n",
    "        - Ensures `freqs_cis` can be broadcast across `x`.\n",
    "        - Constructs a shape list where only the dimensions corresponding to the sequence length and feature size are retained, others are set to `1`.\n",
    "    - **Reshaping**:\n",
    "        - Reshapes `freqs_cis` to match the required broadcast shape.\n",
    "\n",
    "3. **Applying Rotary Embeddings (`apply_rotary_emb`)**:\n",
    "    - **Parameters**:\n",
    "        - `xq`, `xk`: Query and key tensors from the attention mechanism.\n",
    "        - `freqs_cis`: The precomputed complex frequency tensor.\n",
    "    - **Complex Conversion**:\n",
    "        - Converts `xq` and `xk` to complex numbers by reshaping the last dimension into pairs, facilitating complex multiplication.\n",
    "    - **Broadcast Adjustment**:\n",
    "        - Adjusts the shape of `freqs_cis` to match `xq` and `xk` using `reshape_for_broadcast`.\n",
    "    - **Rotary Embedding Application**:\n",
    "        - Multiplies the complex queries and keys with the complex frequencies.\n",
    "        - Converts the results back to real numbers and flattens the last two dimensions.\n",
    "    - **Output**:\n",
    "        - Returns the transformed `xq` and `xk` tensors in their original data type.\n",
    "\n",
    "---\n",
    "\n",
    "This trio of functions performs a masterful operation, setting up the intricate dance of frequencies within our model's attention mechanism. Each step is a deliberate move, enhancing the model's ability to capture positional information and contextual relationships, ensuring our model sees not just the present, but the sequence and structure that bind the past and future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_is_precompute_freqs_cis = precompute_freqs_cis(3, 4)\n",
    "print(what_is_precompute_freqs_cis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "what_is_reshape_for_broadcast = reshape_for_broadcast(torch.randn(3, 4), x)\n",
    "print(x)\n",
    "print(what_is_reshape_for_broadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Attention` Class Explanation\n",
    "\n",
    "- **Initialization (`__init__` method)**:\n",
    "  - Defines the number of heads for the multi-head attention mechanism (`n_heads`).\n",
    "  - Sets up key-value heads (`n_kv_heads`) and determines repetitions (`n_rep`).\n",
    "  - Calculates the dimension of each head (`head_dim`).\n",
    "  - Initializes linear layers for queries (`wq`), keys (`wk`), values (`wv`), and output (`wo`).\n",
    "\n",
    "- **Forward Pass (`forward` method)**:\n",
    "  - **Input Transformation**: Transforms the input tensor `x` into queries, keys, and values.\n",
    "  - **Reshape for Multi-head Attention**: Reshapes queries, keys, and values into their respective head dimensions.\n",
    "  - **Rotary Embedding Application**: Applies rotary embeddings to the queries and keys.\n",
    "  - **Attention Calculation**: Computes attention scores using scaled dot-product, applies softmax, and multiplies by values to get the attention output.\n",
    "  - **Output Projection**: Projects the concatenated attention outputs back to the model's dimensionality using `wo`.\n",
    "\n",
    "---\n",
    "\n",
    "### `FeedForward` Class Explanation\n",
    "\n",
    "- **Initialization (`__init__` method)**:\n",
    "  - Sets up the feedforward network with a configurable hidden dimension (`hidden_dim`).\n",
    "  - Adjusts the hidden dimension based on a multiplier (`ffn_dim_multiplier`) and ensures it aligns with a specific multiple (`multiple_of`).\n",
    "  - Initializes three linear layers (`w1`, `w2`, `w3`) for the feedforward transformations.\n",
    "\n",
    "- **Forward Pass (`forward` method)**:\n",
    "  - **Linear Transformations**: Applies the first and third linear transformations (`w1` and `w3`) to the input tensor `x`.\n",
    "  - **Element-wise Multiplication**: Multiplies the outputs of `w1` and `w3`.\n",
    "  - **Activation Function**: Applies the SiLU (Sigmoid Linear Unit) activation function to the multiplied output.\n",
    "  - **Final Transformation**: Transforms the activated output through the second linear layer (`w2`).\n",
    "\n",
    "---\n",
    "\n",
    "### `TransformerBlock` Class Explanation\n",
    "\n",
    "- **Initialization (`__init__` method)**:\n",
    "  - Sets up the transformer block with a specific layer ID.\n",
    "  - Initializes RMS normalization layers for attention and feedforward networks.\n",
    "  - Integrates the `Attention` and `FeedForward` modules to handle respective transformations.\n",
    "\n",
    "- **Forward Pass (`forward` method)**:\n",
    "  - **Attention Normalization**: Normalizes the input tensor `x` before passing it through the attention module.\n",
    "  - **Residual Connection**: Adds the attention output back to the input tensor.\n",
    "  - **Feedforward Normalization**: Normalizes the result before passing it through the feedforward network.\n",
    "  - **Final Residual Connection**: Adds the feedforward output back to the tensor, completing the transformer block processing.\n",
    "\n",
    "---\n",
    "\n",
    "### `Transformer` Class Explanation\n",
    "\n",
    "- **Initialization (`__init__` method)**:\n",
    "  - Defines the token embedding layer using the vocabulary size and model dimensionality.\n",
    "  - Creates a list of transformer blocks, iterating up to the defined number of layers (`n_layers`).\n",
    "  - Initializes the final normalization layer and output linear layer.\n",
    "  - Precomputes frequency components for rotary embeddings.\n",
    "\n",
    "- **Forward Pass (`forward` method)**:\n",
    "  - **Token Embedding**: Converts input tokens to embeddings.\n",
    "  - **Frequency Component Preparation**: Prepares frequency components for the current sequence length.\n",
    "  - **Attention Masking**: Creates an attention mask to manage sequence dependencies.\n",
    "  - **Layer Processing**: Passes the embeddings through each transformer block sequentially.\n",
    "  - **Final Normalization and Output Projection**: Normalizes the final layer output and projects it to the vocabulary space.\n",
    "\n",
    "- **Text Generation (`generate` method)**:\n",
    "  - **Iterative Generation**: Generates new tokens by iteratively applying the model to the most recent sequence.\n",
    "  - **Probability Sampling**: Uses softmax to convert logits to probabilities and samples the next token.\n",
    "  - **Sequence Extension**: Extends the input sequence with the newly generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_heads if n_kv_heads is None else n_kv_heads\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(n_heads * self.head_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        B, L, D = x.shape\n",
    "        queries, keys, values = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.queries(x)\")\n",
    "                createPlot(queries.detach(), title=\"Queries\")\n",
    "                print(\"self.keys(x)\")\n",
    "                createPlot(keys.detach(), title=\"Keys\")\n",
    "                print(\"self.values(x)\")\n",
    "                createPlot(values.detach(), title=\"Values\")\n",
    "\n",
    "        queries = queries.view(B, L, self.n_heads, self.head_dim)\n",
    "        keys = keys.view(B, L, self.n_kv_heads, self.head_dim)\n",
    "        values = values.view(B, L, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        queries, values = apply_rotary_emb(queries, keys, freqs_cis=freqs_cis)\n",
    "\n",
    "        queries = queries.transpose(1, 2)  # (bs, n_heads, L, head_dim)\n",
    "        keys = keys.transpose(1, 2) # (bs, n_heads, cache_len + L, head_dim)\n",
    "        values = values.transpose(1, 2) # (bs, n_heads, cache_len + L, head_dim)\n",
    "        scores = torch.matmul(queries, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                plot_intermediate_attention(scores)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_heads, L, cache_len + L)\n",
    "\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(queries)\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                plot_intermediate_attention(scores)\n",
    "\n",
    "        output = torch.matmul(scores, values)  # (bs, n_heads, L, head_dim)\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                plot_intermediate_attention(output, title=\"Attention Output\", xlabel=\"Scores\", ylabel=\"Values\")\n",
    "        output = output.transpose(1, 2).contiguous().view(B, L, -1)\n",
    "\n",
    "        wo = self.wo(output)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.wo(output)\")\n",
    "                createPlot(wo.detach(), title=\"Output Projection\")\n",
    "        return wo\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w1 = self.w1(x)\n",
    "        w3 = self.w3(x)\n",
    "        multiplied = w1 * w3\n",
    "        activated = F.silu(multiplied)\n",
    "        w2 = self.w2(activated)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.w1(x)\")\n",
    "                createPlot(w1.detach(), title=\"w1\")\n",
    "                print(\"self.w3(x)\")\n",
    "                createPlot(w3.detach(), title=\"w3\")\n",
    "                print(\"w1 * w3\")\n",
    "                createPlot(multiplied.detach(), title=\"multiplied\")\n",
    "                print(\"F.silu(multiplied)\")\n",
    "                createPlot(activated.detach(), title=\"activated\")\n",
    "                print(\"self.w2(activated)\")\n",
    "                createPlot(w2.detach(), title=\"w2\")\n",
    "        return w2\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int):\n",
    "        super().__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.dim = dim\n",
    "        self.attention_norm = RMSNorm(self.dim, eps=rms_norm_eps)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.ffn_norm = RMSNorm(self.dim, eps=rms_norm_eps)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=dim,\n",
    "            hidden_dim=4 * dim,\n",
    "            multiple_of=multiple_of,\n",
    "            ffn_dim_multiplier=ffn_dim_multiplier,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        if plot:\n",
    "            print(f\"START Transformer Block {self.layer_id}\")\n",
    "        attention_norm = self.attention_norm(x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.attention_norm(x)\")\n",
    "                createPlot(attention_norm.detach(), title=\"attention_norm\")\n",
    "\n",
    "        res1 = x + self.attention(attention_norm, freqs_cis, mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"x + self.attention(attention_norm, freqs_cis, mask)\")\n",
    "                createPlot(res1.detach(), title=\"x + self.attention(attention_norm, freqs_cis, mask)\")\n",
    "\n",
    "        ffn_norm = self.ffn_norm(res1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.ffn_norm(res1)\")\n",
    "                createPlot(ffn_norm.detach(), title=\"self.ffn_norm(res1)\")\n",
    "\n",
    "        out = res1 + self.feed_forward(ffn_norm)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"res1 + self.feed_forward(ffn_norm)\")\n",
    "                createPlot(out.detach(), title=\"res1 + self.feed_forward(ffn_norm)\")\n",
    "                print(f\"END Transformer Block {self.layer_id}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embeddings = nn.Embedding(vocab_size, dim, padding_idx=pad_id)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id))\n",
    "\n",
    "        self.norm = RMSNorm(dim, eps=rms_norm_eps)\n",
    "        self.output = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(dim // n_heads, max_seq_len * 2)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int = 0, targets=None):\n",
    "        B, L = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.tok_embeddings(tokens)\")\n",
    "                createPlot(h.detach(), title=\"tok_embeddings\")\n",
    "\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + L]\n",
    "\n",
    "        mask = None\n",
    "        if L > 1:\n",
    "            mask = torch.full((L, L), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            mask = torch.hstack([torch.zeros((L, start_pos), device=tokens.device), mask]).type_as(h)\n",
    "\n",
    "        if plot:\n",
    "            plot_mask_tensor(mask)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cis, mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"h = layer(h, freqs_cis, mask)\")\n",
    "                createPlot(h.detach(), title=\"h = layer(h, freqs_cis, mask)\")\n",
    "\n",
    "        h = self.norm(h)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"self.norm(h)\")\n",
    "                createPlot(h.detach(), title=\"self.norm(h)\")\n",
    "\n",
    "        logits = self.output(h).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if plot:\n",
    "                print(\"logits = self.output(h).float()\")\n",
    "                plot_lm_head_output(logits.detach(), title=\"self.output(h).float()\")\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, L, D = logits.shape\n",
    "            logits = logits.view(B*L, D)\n",
    "            targets = targets.view(B*L)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -max_seq_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            if plot:\n",
    "                plot_probs_or_logits(logits, title=\"Logits from the Last Dimension\", xlabel=\"Sequence Length (Tokens)\", ylabel=\"Logit Value\", label=\"Logits\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if plot:\n",
    "                plot_probs_or_logits(probs)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Tutal Parameters: {total_params}\")\n",
    "\n",
    "save_model_parameters_to_file(model, 'model_untrained_parameters.txt')\n",
    "parameters = parse_parameters_from_file('model_untrained_parameters.txt')\n",
    "visualize_parameters(parameters, 'all_untrained_parameters_visualization.png', 'individual_untrained_parameter_plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Untrained Model\n",
    "\n",
    "First, let's prepare our input and tokens, then pass them through the untrained model to observe its raw, initial output.\n",
    "\n",
    "1. **Tokenize the Input**:\n",
    "   - Take the given line from Shakespeare and convert it into a sequence of tokens using our tokenizer.\n",
    "\n",
    "2. **Create Tensor from Tokens**:\n",
    "   - Convert the sequence of tokens into a tensor format suitable for model input.\n",
    "\n",
    "3. **Print the Tokens**:\n",
    "   - Output the token tensor to ensure our input is correctly formatted.\n",
    "\n",
    "Here's how it would unfold.\n",
    "\n",
    "- **Line Selection**: We select a specific line from \"Romeo and Juliet\" to test the model's initial output. The line I chose \"ROMEO:\n",
    "I pay thy poverty, and not thy will.\"\n",
    "- **Tokenization**: The `tokenize` function transforms the text into a sequence of integers, each representing a character.\n",
    "- **Tensor Conversion**: The token sequence is converted into a tensor and reshaped to match the model's expected input format (batch size of 1).\n",
    "- **Output Display**: We print the resulting tensor to verify that the tokenization process was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First untrained testing\")\n",
    "\n",
    "line_19826 = \"\"\"ROMEO:\\nI pay thy poverty, \"\"\"\n",
    "\n",
    "first_tokens = tokenizer.encode(line_19826)\n",
    "print(first_tokens.tokens)\n",
    "first_input = torch.LongTensor(first_tokens.ids).unsqueeze(0)\n",
    "print(first_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Untrained Model with Plots\n",
    "\n",
    "Here's how we can set up the test and generate the output while enabling plots for each step of the process.\n",
    "\n",
    "1. **Plotting Enabled**:\n",
    "   - Set `plot = True` to ensure that plots are generated at each stage of the model's forward pass and generation process.\n",
    "\n",
    "2. **Generate New Token**:\n",
    "   - Call the `generate` method on the model with the `first_tokens` as input and `max_new_tokens=1` to generate one new token.\n",
    "   \n",
    "3. **Output Conversion**:\n",
    "   - Convert the generated output tensor to a list of tokens.\n",
    "\n",
    "4. **Detokenize**:\n",
    "   - Use the `detokenize` function to convert the list of tokens back into text.\n",
    "\n",
    "5. **Print Output**:\n",
    "   - Print the generated character or token to observe the untrained model's response.\n",
    "\n",
    "### Prepare for Plots:\n",
    "\n",
    "As the model generates the output, the `plot` flag will trigger multiple plots displaying the internal states and transformations at various stages:\n",
    "\n",
    "- **Token Embeddings**: Visualize how the input tokens are converted into embeddings.\n",
    "- **Attention Mechanisms**: Observe the queries, keys, values, and attention scores.\n",
    "- **Feedforward Transformations**: See the intermediate results of the feedforward network.\n",
    "- **Layer Outputs**: Track the output at each layer of the transformer.\n",
    "\n",
    "These plots will provide a comprehensive view of the untrained model's behavior, allowing us to analyze and understand its initial, unbiased responses.\n",
    "\n",
    "Brace yourself for the insights and visualizations that follow, as they will pave the way for informed adjustments and training strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = True\n",
    "output = model.generate(first_input, max_new_tokens=1)\n",
    "out = output[0].tolist() # Uncommend this if you used the Simple tokenizer \n",
    "generated_text_from_untrained = tokenizer.decode(out, skip_special_tokens=True)\n",
    "print(f\"the next generated character or Token is: {generated_text_from_untrained}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate more Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "output = model.generate(first_input, max_new_tokens=10)\n",
    "out = output[0].tolist()\n",
    "generated_text_from_untrained = tokenizer.decode(out, skip_special_tokens=True)\n",
    "print(generated_text_from_untrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Analysis:\n",
    "\n",
    "The output you received:\n",
    "```\n",
    "ROMEO:\n",
    "I pay thy poverty, Q-Vtwm::Hyom3yH3O'NO:AV3uJiuKuQg$nyMBla:uOH'ql?MgIG#rRH:;;lWg;aQFegX!-,p.T.V saWVGAA.jpFw:g!nOVZiBzq\n",
    "```\n",
    "is not ideal and quite different from the expected:\n",
    "```\n",
    "ROMEO:\n",
    "I pay thy poverty, and not thy will.\n",
    "```\n",
    "\n",
    "\n",
    "### Explanation and Next Steps:\n",
    "\n",
    "1. **Untrained Model**:\n",
    "   - The model is currently untrained, meaning it hasn't learned the patterns, syntax, or semantics of the Shakespearean language.\n",
    "   - The output is essentially random, reflecting the lack of training.\n",
    "\n",
    "2. **Training the Model**:\n",
    "   - To generate coherent and contextually accurate text, the model needs to be trained on a substantial amount of relevant data.\n",
    "   - Training involves feeding the model many examples of Shakespearean text (or any desired text corpus) and adjusting its parameters to minimize prediction errors.\n",
    "\n",
    "3. **Steps for Training**:\n",
    "   - **Prepare Dataset**: Ensure you have a large and well-prepared dataset of Shakespearean text.\n",
    "   - **Define Training Loop**: Implement a training loop that iteratively adjusts the model's weights based on the loss computed from its predictions.\n",
    "   - **Evaluation and Fine-tuning**: Regularly evaluate the model's performance and fine-tune hyperparameters for better results.\n",
    "\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The initial output serves as a baseline, highlighting the importance of training. By investing time and computational resources into training the model, it will progressively learn to generate text that aligns with your expectations.\n",
    "\n",
    "If you need further assistance with the training setup or specific aspects of your model, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly, Gökdeniz. Let’s go through the training setup step-by-step, ensuring everything is well-defined and optimized for training your model effectively.\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "Here’s a detailed explanation of each part of the training process:\n",
    "\n",
    "#### Data Preparation\n",
    "1. **Tokenize Dataset**: Convert the entire dataset into a sequence of tokens.\n",
    "2. **Train/Validation Split**: Split the tokenized data into 90% for training and 10% for validation.\n",
    "\n",
    "#### Batch Preparation\n",
    "3. **Get Batch Function**: Define a function to sample batches of data for training and validation. This function randomly selects sequences of length `max_seq_len` from the data.\n",
    "\n",
    "#### Loss Estimation\n",
    "4. **Estimate Loss Function**: Create a function to evaluate the model's performance on both training and validation sets without updating the model parameters (`@torch.no_grad()`).\n",
    "\n",
    "#### Optimizer\n",
    "5. **AdamW Optimizer**: Use AdamW, a variant of the Adam optimizer with weight decay, for better performance on large-scale datasets.\n",
    "\n",
    "#### Training Loop\n",
    "6. **Training Loop**: Train the model for a specified number of steps (`max_steps`). Periodically evaluate and log the training and validation losses.\n",
    "\n",
    "#### Save and Plot\n",
    "7. **Save the Model**: Save the trained model parameters for future use.\n",
    "8. **Plot Loss Curves**: Plot the training and validation loss curves to visualize the training progress.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. **Data Preparation**: Tokenize the dataset and split it into training and validation sets to ensure the model can generalize well.\n",
    "2. **Batch Preparation**: Randomly sample batches for both training and evaluation to prevent overfitting and ensure the model sees a variety of data.\n",
    "3. **Loss Estimation**: Regularly estimate the model’s loss on training and validation data to monitor overfitting and model performance.\n",
    "4. **Optimizer**: Use the AdamW optimizer, which helps in better generalization due to its weight decay property.\n",
    "5. **Training Loop**: Train the model iteratively, evaluate performance periodically, and log the losses for visualization.\n",
    "6. **Save and Plot**: Save the trained model for future use and plot the loss curves to visualize the training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 10000\n",
    "eval_steps = 100\n",
    "eval_interval = 1000\n",
    "lr = 0.002\n",
    "max_batch_size = 32\n",
    "\n",
    "steps = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"... Loading Dataset\")\n",
    "with open(\"/Users/gokdenizgulmez/Desktop/Inside-Llama/tiny-shakespear.txt\", \"r\") as file:\n",
    "    dataset = file.read()\n",
    "\n",
    "# Tokenizing the dataset\n",
    "dataset = tokenizer.encode(dataset)\n",
    "# Converting tokens to tensor and adding batch dimension\n",
    "data = torch.LongTensor(dataset.ids).unsqueeze(0)\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "n = int(0.9 * len(data[0]))\n",
    "train_data = data[:, :n]  # Ensure correct slicing\n",
    "val_data = data[:, n:]    # Ensure correct slicing\n",
    "\n",
    "print(\"Dataset loaded and split into training and validation sets.\")\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    if len(data[0]) <= max_seq_len:\n",
    "        raise ValueError(f\"Data length ({len(data[0])}) is not sufficient for the sequence length ({max_seq_len}).\")\n",
    "    ix = torch.randint(len(data[0]) - max_seq_len, (max_batch_size,))\n",
    "    x = torch.stack([data[0, i:i+max_seq_len] for i in ix])\n",
    "    y = torch.stack([data[0, i+1:i+max_seq_len+1] for i in ix])\n",
    "    return x.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_steps)\n",
    "        for k in range(eval_steps):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, start_pos=0, targets=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "print(\"Training\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(max_steps - 1):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if epoch % eval_interval == 0 or epoch == max_steps - 1:\n",
    "        losses = estimate_loss()\n",
    "        steps.append(epoch)\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, start_pos=0, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"trained_llama3_model.pth\")\n",
    "\n",
    "# Plot the loss curves\n",
    "createLossPlot(steps, train_losses)\n",
    "createLossPlot(steps, val_losses, title=\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets now test the trained Llama model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teh saved mdoel weights\n",
    "trained_model = Transformer()\n",
    "trained_model.load_state_dict(torch.load(\"trained_llama3_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_parameters_to_file(model, 'model_trained_parameters.txt')\n",
    "parameters = parse_parameters_from_file('model_trained_parameters.txt')\n",
    "visualize_parameters(parameters, 'all_trained_parameters_visualization.png', 'individual_trained_parameter_plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "output = model.generate(first_input, max_new_tokens=10)\n",
    "out = output[0].tolist()\n",
    "generated_text_from_untrained = tokenizer.decode(out, skip_special_tokens=True)\n",
    "print(generated_text_from_untrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = True\n",
    "output = trained_model.generate(first_input, max_new_tokens=1)\n",
    "out = output[0].tolist()\n",
    "generated_text = tokenizer.decode(out, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
