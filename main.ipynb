{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Introduction\n",
    "\n",
    "Welcome to the labyrinth of \"Inside Llama,\" where we unravel the complexities of Meta's Llama 3 model. This notebook is a testament to the power of precision, knowledge, and the relentless pursuit of perfection. It is designed for those who aspire not just to understand, but to master the inner workings of one of the most sophisticated language models in existence.\n",
    "\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook is divided into several critical sections. Each one is a rung on the ladder to dominance over the machine learning landscape.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "Our mission is clear: to construct and train the Llama 3 model from scratch, employing a character-based tokenizer as our tool of choice. Inspired by the teachings of Andrew Karpathy, this tokenizer is the key to unlocking the full potential of our model. Should your preferences lean towards tradition, the original tokenizer from the Huggingface Hub is but a switch away, courtesy of the `transformers` library.\n",
    "\n",
    "\n",
    "### Architectural Blueprint\n",
    "\n",
    "Before we embark on our journey, let us pause to appreciate the architectural elegance of Llama 3, as depicted in the following diagram:\n",
    "\n",
    "<img src=\"./Llama-Architecture.png\" alt=\"Llama Architecture\" width=\"500\">\n",
    "\n",
    "\n",
    "This diagram is more than a mere image; it is a blueprint of our conquest:\n",
    "\n",
    "1. **Input Tokens**: The journey begins with the input tokens, the raw material fed into the model.\n",
    "\n",
    "2. **Embeddings**: These tokens are transformed into embeddings, the foundation upon which our model is built.\n",
    "\n",
    "3. **Transformer Block**: The core of our architecture, the Transformer Block, where the magic happens:\n",
    "   - **Multi-Head-Self-Attention**: This component employs Grouped-Multi-Query-Attention with KV-Cache, a sophisticated mechanism for attention across multiple heads. We will not be using the KV-Cache in this notebook.\n",
    "   - **RMS Norm**: Normalization is applied to ensure stability and efficiency.\n",
    "   - **SwiGLU Activated MLP Layer**: A crucial layer where activation functions breathe life into our model.\n",
    "   - **RMS Norm**: Another layer of normalization to maintain equilibrium.\n",
    "   - **Residual Connections**: These connections ensure that information flows smoothly through the network without loss.\n",
    "   \n",
    "4. **Output Tokens**: The culmination of our efforts, the output tokens, are derived from the softmax probabilities and argmax operations, translating the model's predictions into tangible results.\n",
    "\n",
    "This is the architecture that will guide us, the blueprint that will lead us to mastery. As we proceed through the notebook, each section will bring us closer to fully understanding and harnessing the power of Llama 3.\n",
    "\n",
    "Prepare yourself for a journey of discovery, precision, and unparalleled mastery. Welcome to \"Inside Llama\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m createPlot, createLossPlot, LlamaVisualizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plot'"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from plot import createPlot, createLossPlot, LlamaVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "\n",
    "- **dim: int = 16 # 4096**: The core dimensions of our model. In the realm of possibilities, we set it to 16, though it could scale to 4096 in more ambitious undertakings like the Llama3 8B model.\n",
    "\n",
    "- **n_layers: int = 6 # 32**: The number of Decoder-Transformer Layers. We start with 6, but the ceiling is our device were training it on, Llama3 8B uses 32.\n",
    "\n",
    "- **n_heads: int = 8 # 32**: The count of Single Attention Heads in our Multi-Head Attention mechanism. We begin with 8, yet can extend to Llama3 8B's 32, each head enhancing our model's perceptive power.\n",
    "\n",
    "- **n_kv_heads: Optional[int] = 8 # 8**: The number of key-value heads in the attention mechanism. Set at 8, a balanced number ensuring efficiency and depth.\n",
    "\n",
    "- **vocab_size: int = -1**: The vocabulary size, as yet undefined.\n",
    "\n",
    "- **multiple_of: int = 24**: This ensures the SwiGLU hidden layer size is a multiple of a large power of 2, originally 256. It’s a move of strategic alignment, ensuring optimal performance.\n",
    "\n",
    "- **ffn_dim_multiplier: Optional[float] = None**: The multiplier for the Feed-Forward Network dimension, currently undefined, allowing for dynamic scaling as needed.\n",
    "\n",
    "- **rms_norm_eps: float = 1e-5**: The epsilon value for RMS normalization, a fine-tuned parameter ensuring stability and precision in our model’s calculations.\n",
    "\n",
    "- **max_batch_size: int = 6**: The maximum batch size, set to 6. A modest start, with the potential for scaling as our model’s appetite grows.\n",
    "\n",
    "- **max_seq_len: int = 32**: The maximum sequence length, a defining parameter that caps our input sequences at 32, ensuring manageable complexity.\n",
    "\n",
    "- **plot = False**: The Plot property, set to False for now. When we choose to visualize the values within our model, we’ll switch it on, revealing the intricate workings beneath the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim: int = 16\n",
    "n_layers: int = 6\n",
    "n_heads: int = 8\n",
    "n_kv_heads: Optional[int] = 8\n",
    "vocab_size: int = -1\n",
    "multiple_of: int = 24\n",
    "ffn_dim_multiplier: Optional[float] = None\n",
    "rms_norm_eps: float = 1e-5\n",
    "max_batch_size: int = 6\n",
    "max_seq_len: int = 32\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's shift our attention (pun intended) to our Tokenizer and our Dataset.\n",
    "\n",
    "In this segment, we see the elegance of our strategy unfold:\n",
    "\n",
    "1. **Loading the Dataset**: We begin by drawing in our raw data, a fundamental step that brings us closer to the heart of our endeavor.\n",
    "\n",
    "2. **Creating the Tokenizer**: \n",
    "    - **Character Analysis**: We meticulously analyze the unique characters within our dataset, understanding the building blocks of our linguistic universe.\n",
    "    - **Vocab Insights**: We reveal the vocabulary size, a metric of our model’s breadth and comprehension.\n",
    "    - **Mapping Characters**: Two critical dictionaries are crafted:\n",
    "        - **`stoi`**: Maps characters to their respective indices.\n",
    "        - **`itos`**: Reverses the map, from indices back to characters.\n",
    "    - **Tokenization and Detokenization**: We define our lambda functions, transforming strings to sequences of indices and back, ensuring seamless transitions between raw text and numerical representations.\n",
    "\n",
    "3. **Padding ID**: Finally, we identify our padding ID, a key player in managing sequences of varying lengths, ensuring consistency and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... Loading Dataset\")\n",
    "with open(\"dataset.txt\", \"r\") as file:\n",
    "    dataset = file.read()\n",
    "\n",
    "print(\"... Creating Tokenizer\")\n",
    "chars = sorted(list(set(dataset)))\n",
    "print(\".......................................................\")\n",
    "print(f\"Learned Chars: {chars}\")\n",
    "print(\".......................................................\")\n",
    "chars_dataset = sorted(list(set(dataset)))\n",
    "vocab_size = len(chars_dataset)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "tokenize = lambda s: [stoi[c] for c in s]\n",
    "detokenize = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "pad_id = tokenize(\"P\")[0]\n",
    "print(f\"pading ID: {pad_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
